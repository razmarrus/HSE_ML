{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will work with **logistic regression**, **l2-regularisation** and **gradient descent**. Main point of this task is to show how different parameters affect training.  \n",
    "Original seven tasks will be presented in Russian.\n",
    "\n",
    "**Tasks will be duplicated in english further in this assignment**\n",
    "\n",
    "1. Загрузите данные из файла data-logistic.csv. Это двумерная выборка, целевая переменная на которой принимает значения -1 или 1.\n",
    "2. Убедитесь, что выше выписаны правильные формулы для градиентного спуска. Обратите внимание, что мы используем полноценный градиентный спуск, а не его стохастический вариант!\n",
    "3. Реализуйте градиентный спуск для обычной и L2-регуляризованной (с коэффициентом регуляризации 10) логистической регрессии. Используйте длину шага k=0.1. В качестве начального приближения используйте вектор (0, 0).\n",
    "4. Запустите градиентный спуск и доведите до сходимости (евклидово расстояние между векторами весов на соседних итерациях должно быть не больше 1e-5). Рекомендуется ограничить сверху число итераций десятью тысячами.\n",
    "5. Какое значение принимает AUC-ROC на обучении без регуляризации и при ее использовании? Эти величины будут ответом на задание. В качестве ответа приведите два числа через пробел. Обратите внимание, что на вход функции roc_auc_score нужно подавать оценки вероятностей, подсчитанные обученным алгоритмом. Для этого воспользуйтесь сигмоидной функцией: a(x) = 1 / (1 + exp(-w1 x1 - w2 x2)). \n",
    "6. Попробуйте поменять длину шага. Будет ли сходиться алгоритм, если делать более длинные шаги? Как меняется число итераций при уменьшении длины шага?\n",
    "7. Попробуйте менять начальное приближение. Влияет ли оно на что-нибудь?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task №1\n",
    "*Ru*: Загрузите данные из файла data-logistic.csv. Это двумерная выборка, целевая переменная на которой принимает значения -1 или 1.\n",
    "\n",
    "*En:* Load data from data-logistic.csv file. This is a two-dimensional dataframe, the target variable takes values -1 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.663827</td>\n",
       "      <td>-0.138526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.994596</td>\n",
       "      <td>2.468025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.247395</td>\n",
       "      <td>0.749425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.309374</td>\n",
       "      <td>1.899836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.849143</td>\n",
       "      <td>2.407750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1</td>\n",
       "      <td>4.245176</td>\n",
       "      <td>3.053931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>2.437935</td>\n",
       "      <td>1.357804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.876638</td>\n",
       "      <td>1.533398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1</td>\n",
       "      <td>-6.824446</td>\n",
       "      <td>-13.934211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.865147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1          2\n",
       "0   -1 -0.663827  -0.138526\n",
       "1    1  1.994596   2.468025\n",
       "2   -1 -1.247395   0.749425\n",
       "3    1  2.309374   1.899836\n",
       "4    1  0.849143   2.407750\n",
       "..  ..       ...        ...\n",
       "200  1  4.245176   3.053931\n",
       "201  1  2.437935   1.357804\n",
       "202 -1 -1.876638   1.533398\n",
       "203  1 -6.824446 -13.934211\n",
       "204 -1  0.001805   0.865147\n",
       "\n",
       "[205 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv('data-logistic.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[1, 2]]\n",
    "y = df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task №2\n",
    "*Ru*: Убедитесь, что выше выписаны правильные формулы для градиентного спуска. Обратите внимание, что мы используем полноценный градиентный спуск, а не его стохастический вариант!\n",
    "\n",
    "*En:* Make sure the formulas, that are written above, are correct for gradient descent. Please note that we are using full gradient descent, not it's stochastic version! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't rewrite formulas here. They will appear in the next task in python code anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task №3\n",
    "*Ru*: Реализуйте градиентный спуск для обычной и L2-регуляризованной (с коэффициентом регуляризации 10) логистической регрессии. Используйте длину шага k=0.1. В качестве начального приближения используйте вектор (0, 0).\n",
    "\n",
    "*En:* Implement gradient descent for logistic regression with L2-regularized and without it (with a regularization factor of 10) . Use stride length k = 0.1. Use the vector (0, 0) as an initial guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_step(w1, w2, x, y, k, C):\n",
    "    sum_1 = 0\n",
    "    sum_2 = 0\n",
    "    l = len(y)\n",
    "    \n",
    "    for i in range(1, len(y)):\n",
    "        denominator = 1.0 + math.exp(-y[i]*(w1*x[1][i]+w2*x[2][i]))\n",
    "        sum_1 +=y[i]*x[1][i]*(1.0 - 1.0/ denominator)\n",
    "        sum_2 +=y[i]*x[2][i]*(1.0 - 1.0/ denominator)\n",
    "    w1 = w1 + k/l*sum_1 - k*C*w1\n",
    "    w2 = w2 + k/l*sum_2 - k*C*w2\n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(w1, w2, X, y, k=0.1, C=0.0, i_max = 10000):\n",
    "    i = 0\n",
    "    e = 1\n",
    "    w1_prev, w2_prev = w1, w2\n",
    "    while i < i_max: \n",
    "\n",
    "        w1, w2 = grad_step(w1, w2, X, y, k, C)\n",
    "        i = i + 1\n",
    "        e = math.sqrt((w1 - w1_prev) **2 + (w2 - w2_prev) ** 2)\n",
    "\n",
    "        if i >= i_max or e <= err:\n",
    "            print(\"stopped at\", i,\"-th step; error:\", e)\n",
    "            break\n",
    "            \n",
    "        w1_prev, w2_prev = w1, w2\n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task №4\n",
    "*Ru*: Запустите градиентный спуск и доведите до сходимости (евклидово расстояние между векторами весов на соседних итерациях должно быть не больше 1e-5). Рекомендуется ограничить сверху число итераций десятью тысячами.\n",
    "\n",
    "*En:* Start gradient descent and bring it to convergence (the Euclidean distance between the weight vectors at adjacent iterations should be no more than 1e-5). We strongly recommend to limit the number of iterations from above to ten thousand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0.1\n",
    "#C = 10\n",
    "err=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped at 8 -th step; error: 4.756507634923105e-06\n"
     ]
    }
   ],
   "source": [
    "w1 = 0.0 \n",
    "w2 = 0.0\n",
    "w1, w2 = grad(w1, w2, X, y, C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task №5\n",
    "*Ru*:  Какое значение принимает AUC-ROC на обучении без регуляризации и при ее использовании? Эти величины будут ответом на задание. В качестве ответа приведите два числа через пробел. Обратите внимание, что на вход функции roc_auc_score нужно подавать оценки вероятностей, подсчитанные обученным алгоритмом. Для этого воспользуйтесь сигмоидной функцией: a(x) = 1 / (1 + exp(-w1 x1 - w2 x2)).\n",
    "\n",
    "*En:* What is the value of AUC-ROC in learning without regularization and when using it? These values will be the answer to the task. Give two numbers separated by spaces as your answer. Please note that the input to the roc_auc_score function must be fed with the probability estimates calculated by the trained algorithm. To do this, use the sigmoid function: a (x) = 1 / (1 + exp (-w1 x1 - w2 x2)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped at 242 -th step; error: 9.976423992396325e-06\n"
     ]
    }
   ],
   "source": [
    "w1_no_reg = 0.0 \n",
    "w2_no_reg = 0.0\n",
    "w1_no_reg, w2_no_reg = grad(w1_no_reg, w2_no_reg, X, y)#, i_max = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x1, x2, w1, w2):\n",
    "    return 1.0 / (1.0 + np.exp(-w1 * x1 - w2 * x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49442612036401307,\n",
       " 0.5294178189380503,\n",
       " 0.49577735361297154,\n",
       " 0.5281407094654043,\n",
       " 0.5209280402798716,\n",
       " 0.5062123969344519,\n",
       " 0.529995933939072,\n",
       " 0.5086149245206256,\n",
       " 0.48877075406117704,\n",
       " 0.5175442106313002,\n",
       " 0.5019202478634894,\n",
       " 0.501553008062863,\n",
       " 0.49341311000307037,\n",
       " 0.5055342177599863,\n",
       " 0.5222224326728103,\n",
       " 0.5043043993033314,\n",
       " 0.49296584742701205,\n",
       " 0.519065674316323,\n",
       " 0.5218788325797213,\n",
       " 0.5347792720968784,\n",
       " 0.5021535015173221,\n",
       " 0.38176121491038956,\n",
       " 0.48735244111190906,\n",
       " 0.49208884520213136,\n",
       " 0.5119816271438054,\n",
       " 0.5039325671210118,\n",
       " 0.530609476817534,\n",
       " 0.540964261727246,\n",
       " 0.498410031187921,\n",
       " 0.5135391993740387,\n",
       " 0.49452727502309857,\n",
       " 0.5024899130876974,\n",
       " 0.5458753735213967,\n",
       " 0.5324987215395665,\n",
       " 0.5073916783589336,\n",
       " 0.5136998109575631,\n",
       " 0.534095126999273,\n",
       " 0.5229367659202262,\n",
       " 0.5071602620444648,\n",
       " 0.5237367673822285,\n",
       " 0.5182126380893908,\n",
       " 0.49708153257632526,\n",
       " 0.512660899661073,\n",
       " 0.49239676092040796,\n",
       " 0.507025917044371,\n",
       " 0.5196674215383846,\n",
       " 0.5437305506462128,\n",
       " 0.49728197632637755,\n",
       " 0.5056569944534409,\n",
       " 0.5143547608215426,\n",
       " 0.5186760369992538,\n",
       " 0.5200123638789896,\n",
       " 0.5055180750563512,\n",
       " 0.5302452316621362,\n",
       " 0.4890737017063942,\n",
       " 0.5057957234639888,\n",
       " 0.5055612258557831,\n",
       " 0.5317317809449055,\n",
       " 0.5085774697027564,\n",
       " 0.531821572092354,\n",
       " 0.524522470088637,\n",
       " 0.5153164867147999,\n",
       " 0.5284484955185097,\n",
       " 0.5310334832256061,\n",
       " 0.5037242175358838,\n",
       " 0.5258504637515122,\n",
       " 0.5045892429422135,\n",
       " 0.3179047581973417,\n",
       " 0.5082437446269308,\n",
       " 0.5305984265677072,\n",
       " 0.5117506537411909,\n",
       " 0.5089434644992827,\n",
       " 0.5400550455086129,\n",
       " 0.4930207086767043,\n",
       " 0.4896176388707771,\n",
       " 0.4972631819730994,\n",
       " 0.527701407183038,\n",
       " 0.5118192677391756,\n",
       " 0.5315175548027328,\n",
       " 0.5109210237472064,\n",
       " 0.5357053040569071,\n",
       " 0.48810521948661356,\n",
       " 0.525015457444702,\n",
       " 0.5248489628849929,\n",
       " 0.5102476830808511,\n",
       " 0.541581582139283,\n",
       " 0.5120781898746651,\n",
       " 0.5040010877207279,\n",
       " 0.5054596982363004,\n",
       " 0.49417118460250276,\n",
       " 0.5128622977633676,\n",
       " 0.49291573386346205,\n",
       " 0.525611909583679,\n",
       " 0.5057894772571779,\n",
       " 0.4921708304244668,\n",
       " 0.5238732218121646,\n",
       " 0.5125525815563591,\n",
       " 0.499274536527572,\n",
       " 0.4935024946577326,\n",
       " 0.5206137818048931,\n",
       " 0.5273977596156291,\n",
       " 0.5161947876676884,\n",
       " 0.4953220631447736,\n",
       " 0.5055436215033031,\n",
       " 0.4866605055396489,\n",
       " 0.507512929770203,\n",
       " 0.5185097612307444,\n",
       " 0.5358483826139641,\n",
       " 0.5232136240059218,\n",
       " 0.49713920228327785,\n",
       " 0.48829432811817036,\n",
       " 0.5339596722226131,\n",
       " 0.5258772695649411,\n",
       " 0.5196983390599016,\n",
       " 0.5148440039561188,\n",
       " 0.3934729341977226,\n",
       " 0.5060042190426027,\n",
       " 0.5010579391003571,\n",
       " 0.5296010103296124,\n",
       " 0.4939398875810713,\n",
       " 0.5338928174642444,\n",
       " 0.5035882667836635,\n",
       " 0.5238877187119575,\n",
       " 0.4928308932842202,\n",
       " 0.5324573627410043,\n",
       " 0.5253254292198125,\n",
       " 0.5377197623667199,\n",
       " 0.5130397739375552,\n",
       " 0.5246232489950288,\n",
       " 0.30910221654353665,\n",
       " 0.5264370819701195,\n",
       " 0.5144726882220514,\n",
       " 0.4953929723746231,\n",
       " 0.5015911524709166,\n",
       " 0.543079947854752,\n",
       " 0.48851058190119506,\n",
       " 0.5415613779731329,\n",
       " 0.511097189493649,\n",
       " 0.5433585232595474,\n",
       " 0.5046712137548198,\n",
       " 0.5337536168263004,\n",
       " 0.5292954577206134,\n",
       " 0.5278338564793065,\n",
       " 0.5110940386299774,\n",
       " 0.5175590829391359,\n",
       " 0.5283943349485832,\n",
       " 0.5189123644254297,\n",
       " 0.5136177645042307,\n",
       " 0.5271928507437179,\n",
       " 0.5372884225204837,\n",
       " 0.5008577858668922,\n",
       " 0.4928088390433861,\n",
       " 0.5144051776032341,\n",
       " 0.4902878483174814,\n",
       " 0.5309488371689824,\n",
       " 0.4878221731188925,\n",
       " 0.4842341273951619,\n",
       " 0.5276145697628555,\n",
       " 0.5054444471272652,\n",
       " 0.4981006629685635,\n",
       " 0.48689714352379365,\n",
       " 0.48375675428013054,\n",
       " 0.5238087983949204,\n",
       " 0.5211029287616394,\n",
       " 0.5267490196664645,\n",
       " 0.5325257610213232,\n",
       " 0.4951048797339684,\n",
       " 0.5144138839220301,\n",
       " 0.5071283720101303,\n",
       " 0.48499805883630087,\n",
       " 0.5334263919259226,\n",
       " 0.5027574186120639,\n",
       " 0.5131854090008828,\n",
       " 0.49320011764635824,\n",
       " 0.5238222762350869,\n",
       " 0.5182222391107482,\n",
       " 0.5067349721199725,\n",
       " 0.5229321507495872,\n",
       " 0.4972754711972258,\n",
       " 0.5343800063141433,\n",
       " 0.5345468244867654,\n",
       " 0.5063719233540028,\n",
       " 0.5263042586939876,\n",
       " 0.4972942354970087,\n",
       " 0.5166726706387474,\n",
       " 0.4977365627620713,\n",
       " 0.5056197636252229,\n",
       " 0.49815943395428935,\n",
       " 0.5037426251424275,\n",
       " 0.49037732159820485,\n",
       " 0.5385269402062025,\n",
       " 0.5228816815217454,\n",
       " 0.5058495309157496,\n",
       " 0.48736136167133487,\n",
       " 0.5138647934473699,\n",
       " 0.5327732872427093,\n",
       " 0.49868766820747723,\n",
       " 0.5294845938808059,\n",
       " 0.530763323484454,\n",
       " 0.4917431339710763,\n",
       " 0.5489127003849774,\n",
       " 0.5257053244998233,\n",
       " 0.4961604204269872,\n",
       " 0.3684122983181165,\n",
       " 0.5053690231941034]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score = []\n",
    "y_score_no_reg = []\n",
    "for i in range(len(X[1][:])):\n",
    "    y_score.append(sigmoid(X[1][i],X[2][i], w1, w2))\n",
    "    y_score_no_reg.append(sigmoid(X[1][i],X[2][i], w1_no_reg, w2_no_reg))\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4497366589508577,\n",
       " 0.6895973242549667,\n",
       " 0.42933862211346197,\n",
       " 0.6973172207161892,\n",
       " 0.6146433698687535,\n",
       " 0.5869187939838589,\n",
       " 0.7011847701195669,\n",
       " 0.5295460059750218,\n",
       " 0.4201081161360099,\n",
       " 0.5960654266030009,\n",
       " 0.5001164639819262,\n",
       " 0.4993641478952111,\n",
       " 0.47866319175040467,\n",
       " 0.5958292860158152,\n",
       " 0.6490170579777824,\n",
       " 0.5675391591887173,\n",
       " 0.47049478902042247,\n",
       " 0.6211094005889904,\n",
       " 0.6686423099522777,\n",
       " 0.7434245640706343,\n",
       " 0.5448145347558913,\n",
       " 0.016008472214288888,\n",
       " 0.41572127663562836,\n",
       " 0.44932074173536257,\n",
       " 0.6115285731617947,\n",
       " 0.5367099874585626,\n",
       " 0.697086908476216,\n",
       " 0.7577630325891505,\n",
       " 0.47182975344129413,\n",
       " 0.5870204363660807,\n",
       " 0.46212671731515587,\n",
       " 0.486096530526979,\n",
       " 0.7847094130217673,\n",
       " 0.7481759271598566,\n",
       " 0.5186343365633651,\n",
       " 0.6585967165192955,\n",
       " 0.7046032930898044,\n",
       " 0.6663614477642374,\n",
       " 0.5824372487834664,\n",
       " 0.6944439206735482,\n",
       " 0.6466886768053987,\n",
       " 0.44138655769140545,\n",
       " 0.5941202133877627,\n",
       " 0.4599802529207984,\n",
       " 0.5306753852612744,\n",
       " 0.641310760817103,\n",
       " 0.8025565634754738,\n",
       " 0.4726228684852375,\n",
       " 0.552515189842177,\n",
       " 0.5557909506284305,\n",
       " 0.649782113155603,\n",
       " 0.5865622237499425,\n",
       " 0.5735511855284766,\n",
       " 0.7074979304674172,\n",
       " 0.43469836323546984,\n",
       " 0.4728354178959442,\n",
       " 0.5046070635092547,\n",
       " 0.7081484330610416,\n",
       " 0.5703259103757297,\n",
       " 0.7017993081960807,\n",
       " 0.6684577367472073,\n",
       " 0.6513227302404816,\n",
       " 0.6959767053361308,\n",
       " 0.7247313779831995,\n",
       " 0.5316141578673127,\n",
       " 0.646618561556411,\n",
       " 0.5362993473596613,\n",
       " 0.005606789901623657,\n",
       " 0.563250937257217,\n",
       " 0.6658787814235777,\n",
       " 0.6405345115571222,\n",
       " 0.5147636933060705,\n",
       " 0.752396186224417,\n",
       " 0.4568929048791797,\n",
       " 0.4331119117517111,\n",
       " 0.44475522496028,\n",
       " 0.7059379956907103,\n",
       " 0.6263510599775702,\n",
       " 0.7007021227949616,\n",
       " 0.559998969813447,\n",
       " 0.7356786094985526,\n",
       " 0.3917923618474248,\n",
       " 0.6863957665993365,\n",
       " 0.66216404523333,\n",
       " 0.5828397944467464,\n",
       " 0.758423487257522,\n",
       " 0.55637464399802,\n",
       " 0.4912739107616746,\n",
       " 0.5250195327983949,\n",
       " 0.5140942878614115,\n",
       " 0.5974172046553242,\n",
       " 0.497813389466646,\n",
       " 0.6709880803080989,\n",
       " 0.5492981118599829,\n",
       " 0.44082755153855313,\n",
       " 0.6249846525041163,\n",
       " 0.622698996998098,\n",
       " 0.44764309411046027,\n",
       " 0.4508704692368767,\n",
       " 0.6311676931101179,\n",
       " 0.6920691977031772,\n",
       " 0.6010836836309325,\n",
       " 0.44543009706619663,\n",
       " 0.5631058660235316,\n",
       " 0.46370775425020044,\n",
       " 0.5258156087287041,\n",
       " 0.636787911940898,\n",
       " 0.7206546488027155,\n",
       " 0.6337042531236595,\n",
       " 0.47834396012453306,\n",
       " 0.41712747598643846,\n",
       " 0.7061288240791799,\n",
       " 0.6644307795010077,\n",
       " 0.5909622054400705,\n",
       " 0.6216143946689252,\n",
       " 0.035780751200578303,\n",
       " 0.5032416894184931,\n",
       " 0.5357678508136414,\n",
       " 0.7249102850105934,\n",
       " 0.4435564809672884,\n",
       " 0.7609041597377961,\n",
       " 0.5085818803267482,\n",
       " 0.6381376600695133,\n",
       " 0.3974847523573337,\n",
       " 0.7031623253174912,\n",
       " 0.6887585347900956,\n",
       " 0.7686207635096262,\n",
       " 0.5375451788518301,\n",
       " 0.6610015673222824,\n",
       " 0.004857358548933153,\n",
       " 0.6769148927797283,\n",
       " 0.6099902607802012,\n",
       " 0.4935717450923876,\n",
       " 0.5605161512097525,\n",
       " 0.78155297372796,\n",
       " 0.4142998584379191,\n",
       " 0.73406556050681,\n",
       " 0.5797395603720471,\n",
       " 0.799177158073652,\n",
       " 0.5438373618096944,\n",
       " 0.6887723293962106,\n",
       " 0.6799921891711417,\n",
       " 0.6634796765871646,\n",
       " 0.5840913340841467,\n",
       " 0.670566840946634,\n",
       " 0.7030014329740071,\n",
       " 0.6249431842138353,\n",
       " 0.6025973267538678,\n",
       " 0.7057596923494935,\n",
       " 0.702476587417354,\n",
       " 0.47274155496482523,\n",
       " 0.45578406859628867,\n",
       " 0.5805298711332285,\n",
       " 0.3824094099901437,\n",
       " 0.7185468249079696,\n",
       " 0.3678069608129761,\n",
       " 0.399733510483279,\n",
       " 0.726577129722073,\n",
       " 0.4977009775663602,\n",
       " 0.4777151641433261,\n",
       " 0.4336244607164463,\n",
       " 0.4004538790605005,\n",
       " 0.649633742951558,\n",
       " 0.6281610497739036,\n",
       " 0.6727578932192523,\n",
       " 0.6902639175573508,\n",
       " 0.5068635306303314,\n",
       " 0.6136638757200167,\n",
       " 0.5750344199198033,\n",
       " 0.38437911666124464,\n",
       " 0.6892744062051608,\n",
       " 0.5417080372099133,\n",
       " 0.6097881235987885,\n",
       " 0.45891550291087396,\n",
       " 0.6835387086301852,\n",
       " 0.5776662250276335,\n",
       " 0.5148717297359227,\n",
       " 0.6616229118292052,\n",
       " 0.5019763948813085,\n",
       " 0.7267744203046224,\n",
       " 0.7070363495027226,\n",
       " 0.5148638547825493,\n",
       " 0.696710364344024,\n",
       " 0.5218617525105865,\n",
       " 0.6077616922680282,\n",
       " 0.5056092807590202,\n",
       " 0.613817504389274,\n",
       " 0.45060527068227435,\n",
       " 0.4962160810350742,\n",
       " 0.4400495083923682,\n",
       " 0.7980056650502668,\n",
       " 0.7154857938982091,\n",
       " 0.5056740794572054,\n",
       " 0.420764546838129,\n",
       " 0.6441168010068115,\n",
       " 0.7060124583765557,\n",
       " 0.5242261813157694,\n",
       " 0.7016649866215555,\n",
       " 0.7307580851653057,\n",
       " 0.4236648002075116,\n",
       " 0.8165379969960583,\n",
       " 0.6943145916266601,\n",
       " 0.40369621674317874,\n",
       " 0.03750318885877907,\n",
       " 0.5203654853305285]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score_no_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362857142857142"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc = roc_auc_score(y, y_score)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9270476190476189"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_no_reg = roc_auc_score(y, y_score_no_reg)\n",
    "auc_no_reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task №6\n",
    "*Ru*: Попробуйте поменять длину шага. Будет ли сходиться алгоритм, если делать более длинные шаги? Как меняется число итераций при уменьшении длины шага?\n",
    "\n",
    "*En:* Try changing your stride length. Will the algorithm converge if you take longer steps? How does the number of iterations change with decreasing step length? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [0.001, 0.002, 0.005, 0.01, 0.05, 0.08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped at 305 -th step; error: 9.90183826733714e-06\n",
      "with k= 0.001 AUC-ROC is 0.936095238095238 \n",
      "\n",
      "stopped at 179 -th step; error: 9.959682035765849e-06\n",
      "with k= 0.002 AUC-ROC is 0.936190476190476 \n",
      "\n",
      "stopped at 85 -th step; error: 9.935920467591153e-06\n",
      "with k= 0.005 AUC-ROC is 0.936190476190476 \n",
      "\n",
      "stopped at 47 -th step; error: 9.657902365290397e-06\n",
      "with k= 0.01 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 10 -th step; error: 4.231489041175181e-06\n",
      "with k= 0.05 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 5 -th step; error: 3.2925590680663877e-06\n",
      "with k= 0.08 AUC-ROC is 0.9362857142857142 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in k_list:\n",
    "    w1, w2 = 0, 0\n",
    "    y_score = []\n",
    "    \n",
    "    w1, w2 = grad(w1, w2, X, y, k=i, C=10)\n",
    "    for j in range(len(X[1][:])):\n",
    "        y_score.append(sigmoid(X[1][j],X[2][j], w1, w2))\n",
    "    auc = roc_auc_score(y, y_score)\n",
    "    print(\"with k=\",i, \"AUC-ROC is\", auc, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сonclusion**: The smaller the stride length, the more steps the algorithm needs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task №7\n",
    "*Ru*: Попробуйте менять начальное приближение. Влияет ли оно на что-нибудь?\n",
    "\n",
    "*En:* Try changing the initial approximation. Does it affect anything? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = [0.001, 0.002, 0.005, 0.01, 0.05, 0.08, 0.1, 0.3, 0.6, 1, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped at 8 -th step; error: 4.57587911795355e-06\n",
      "with w= 0.001 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 8 -th step; error: 4.395026667578739e-06\n",
      "with w= 0.002 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 8 -th step; error: 3.851524963260124e-06\n",
      "with w= 0.005 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 8 -th step; error: 2.9449397162643e-06\n",
      "with w= 0.01 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 8 -th step; error: 3.855519586940872e-06\n",
      "with w= 0.05 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 8 -th step; error: 7.990169957742937e-06\n",
      "with w= 0.08 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 9 -th step; error: 2.751839531089911e-06\n",
      "with w= 0.1 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 9 -th step; error: 6.587731633752266e-06\n",
      "with w= 0.3 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 9 -th step; error: 9.219093788554277e-06\n",
      "with w= 0.6 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 10 -th step; error: 2.7999164799843753e-06\n",
      "with w= 1 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 10 -th step; error: 3.0916131667751155e-06\n",
      "with w= 5 AUC-ROC is 0.9362857142857142 \n",
      "\n",
      "stopped at 10 -th step; error: 3.1032408653689338e-06\n",
      "with w= 10 AUC-ROC is 0.9362857142857142 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in w_list:\n",
    "    y_score = []\n",
    "    \n",
    "    w1, w2 = grad(w, w, X, y, C=10)\n",
    "    for j in range(len(X[1][:])):\n",
    "        y_score.append(sigmoid(X[1][j],X[2][j], w1, w2))\n",
    "    auc = roc_auc_score(y, y_score)\n",
    "    print(\"with w=\",w, \"AUC-ROC is\", auc, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сonclusion:** Seems that the initial approximation does not really affect anything "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
